# SmashUp Igor 双重触发 Bug 反思

## 问题回顾

**现象**：Igor（科学小怪蛋）被消灭后，出现两次"选择随从放置+1力量指示物"的交互。

**根本原因**：架构设计缺陷——`SmashUpEventSystem.afterEvents()` 和 `postProcessSystemEvents` 都会对同一批事件调用 `processDestroyMoveCycle`，导致 Igor 的 onDestroy 被触发两次。

## 审计文档为什么没查出来

### 1. 维度覆盖不足

**现有维度**：
- **D9（幂等与重入）**：关注"重复触发/撤销重做安全"，但过于简略
- **D40（后处理循环事件去重）**：关注"去重集合数据源"，但这是**症状**而非**病因**

**缺失维度**：
- **架构层职责重叠检测**：没有维度检查"多个系统是否对同一批数据执行相同处理"
- **事件流路径审计**：没有维度追踪"一个事件从产生到消费的完整路径，是否被重复处理"
- **系统边界清晰性**：没有维度检查"系统职责是否清晰，是否存在职责重叠"

### 2. 审计粒度问题

**现有审计**：
- 聚焦于**单个函数/单个能力**的正确性
- 检查"这个函数的逻辑对不对"
- 检查"这个能力的实现完不完整"

**缺失审计**：
- **跨系统协作审计**：多个系统如何协作？是否有重复处理？
- **事件流全链路审计**：一个事件从产生到最终生效，经过了哪些系统？每个系统做了什么？
- **架构一致性审计**：新旧系统并存时，职责划分是否清晰？

### 3. 历史债务盲区

**问题根源**：
- 最初 `SmashUpEventSystem` 负责完整的事件后处理
- 后来引入 `postProcessSystemEvents` 统一处理所有系统事件
- 但**忘记移除** `SmashUpEventSystem` 中的后处理逻辑
- 导致两个系统都在做同样的事情

**审计文档的盲区**：
- 没有"重构完整性检查"维度
- 没有"历史债务识别"维度
- 没有"系统演进一致性"维度

## 框架是否需要重构

### 当前架构问题

#### 1. 职责重叠（Critical）

**问题**：
```
SmashUpEventSystem.afterEvents()
  → 处理交互解决产生的事件
  → 调用 processDestroyMoveCycle
  → 调用 filterProtectedReturnEvents
  → 调用 processAffectTriggers

postProcessSystemEvents()
  → 处理系统事件（包括交互解决产生的事件）
  → 调用 processDestroyMoveCycle  ← 重复！
  → 调用 filterProtectedReturnEvents  ← 重复！
  → 调用 processAffectTriggers  ← 重复！
```

**根因**：两个系统都认为自己负责"事件后处理"，但没有明确的边界。

#### 2. 调用时机混乱

**Pipeline 执行顺序**：
```
1. execute() → 返回事件
2. postProcessSystemEvents(事件) → 第一次后处理
3. reduce(事件) → 状态更新
4. afterEvents() → 系统响应
   ↓
   SmashUpEventSystem.afterEvents()
     → 交互解决产生新事件
     → 再次调用 processDestroyMoveCycle  ← 第二次后处理！
   ↓
5. postProcessSystemEvents(新事件) → 第三次后处理！！
```

**问题**：同一批事件可能被后处理 2-3 次。

#### 3. 状态传递复杂

**问题**：
- `SmashUpEventSystem` 返回 `{ state, events }`
- `postProcessSystemEvents` 也返回 `{ matchState, events }`
- 两者都可能创建交互，但谁的交互应该被保留？
- Pipeline 需要合并两者的状态，逻辑复杂且容易出错

### 重构方案评估

#### 方案 A：移除 SmashUpEventSystem 的后处理（已采用）

**优点**：
- 最小改动
- 单一职责：`SmashUpEventSystem` 只做"交互解决 → 领域事件"转换
- 统一入口：所有后处理都在 `postProcessSystemEvents`

**缺点**：
- 破坏了一些测试（需要修复）
- `SmashUpEventSystem` 变成"纯转换器"，失去后处理能力

**评估**：✅ 可行，但需要修复测试

#### 方案 B：移除 postProcessSystemEvents，让 SmashUpEventSystem 全权负责

**优点**：
- `SmashUpEventSystem` 职责完整
- 不需要 Pipeline 调用 `postProcessSystemEvents`

**缺点**：
- 需要修改 Pipeline（影响所有游戏）
- `postProcessSystemEvents` 中的 `MINION_PLAYED` 触发链逻辑需要迁移
- 改动范围大，风险高

**评估**：❌ 不推荐，改动太大

#### 方案 C：明确职责边界，避免重复调用

**设计**：
- `SmashUpEventSystem.afterEvents()`：只处理**交互解决产生的事件**
- `postProcessSystemEvents()`：只处理**命令执行产生的事件**
- Pipeline 根据事件来源决定调用哪个

**优点**：
- 职责清晰
- 两个系统各司其职

**缺点**：
- 需要在事件上标记"来源"（命令 vs 交互）
- Pipeline 逻辑变复杂
- 两个系统的后处理逻辑可能不一致

**评估**：⚠️ 可行但复杂

#### 方案 D：统一后处理接口，去重调用

**设计**：
- 提取后处理逻辑到独立函数 `applyPostProcessing(events, state)`
- `SmashUpEventSystem` 和 `postProcessSystemEvents` 都调用这个函数
- 在函数内部做去重（基于事件 ID 或时间戳）

**优点**：
- 逻辑统一
- 自动去重

**缺点**：
- 需要给事件添加唯一 ID
- 去重逻辑可能影响性能
- 仍然没有解决"为什么要调用两次"的根本问题

**评估**：⚠️ 治标不治本

### 推荐方案

**短期（已实施）**：方案 A - 移除 `SmashUpEventSystem` 的后处理
- 修复当前 bug
- 最小改动
- 需要修复测试

**中期**：优化 Pipeline 调用逻辑
- 明确 `postProcessSystemEvents` 的调用时机
- 文档化事件流路径
- 添加架构测试（检测重复调用）

**长期**：建立架构演进规范
- 文档化 Pipeline 事件处理规范（明确每个阶段的职责）
- 新增系统时的检查清单（避免职责重叠）
- 定期架构审计（每季度检查职责重叠）

## 审计文档改进建议

### 新增维度

#### D41：系统职责重叠检测（强制）

**触发条件**：新增系统、重构现有系统、修复"重复触发"类 bug

**审查方法**：
1. **识别所有处理同类数据的系统**：grep 所有调用相同核心函数的地方
2. **绘制调用链路图**：每个系统在什么时机调用？输入是什么？输出是什么？
3. **检查职责边界**：两个系统是否对同一批数据执行相同处理？
4. **判定标准**：
   - 同一批数据被处理一次 → ✅ 正确
   - 同一批数据被处理多次 → ❌ 职责重叠
5. **输出格式**：
   ```
   系统 A: SmashUpEventSystem.afterEvents()
   调用: processDestroyMoveCycle(交互解决产生的事件)
   时机: Pipeline afterEvents 阶段
   
   系统 B: postProcessSystemEvents()
   调用: processDestroyMoveCycle(所有系统事件)
   时机: Pipeline 步骤 4.5 和步骤 5
   
   判定: ❌ 职责重叠（交互解决产生的事件被处理两次）
   ```

#### D42：事件流全链路审计（强制）

**触发条件**：新增事件类型、修改事件处理逻辑、修复"事件丢失/重复"类 bug

**审查方法**：
1. **选择一个代表性事件**：如 `MINION_DESTROYED`
2. **追踪完整生命周期**：
   - 产生：哪些地方会产生这个事件？
   - 传递：事件如何从产生点传递到消费点？
   - 处理：哪些系统会处理这个事件？处理顺序是什么？
   - 消费：事件最终如何影响状态？
3. **检查重复处理**：同一个事件是否被多个系统处理？
4. **检查遗漏处理**：是否有应该处理但没有处理的系统？

#### D43：重构完整性检查（强制）

**触发条件**：引入新系统替代旧系统、重构现有架构

**审查方法**：
1. **识别新旧系统**：新系统是什么？旧系统是什么？
2. **检查职责迁移**：旧系统的职责是否完全迁移到新系统？
3. **检查遗留代码**：旧系统的代码是否已清理？
4. **检查调用点**：所有调用旧系统的地方是否已更新？
5. **判定标准**：
   - 旧系统完全移除 → ✅ 重构完整
   - 旧系统部分保留 → ⚠️ 需要文档说明原因
   - 新旧系统并存且职责重叠 → ❌ 重构不完整

### 改进现有维度

#### D9（幂等与重入）扩展

**原定义**："重复触发/撤销重做安全？"

**扩展定义**：
- **函数级幂等**：同一函数被重复调用是否安全？
- **系统级幂等**：同一批数据被多个系统处理是否安全？
- **架构级幂等**：事件流中是否存在重复处理的路径？

**审查方法**：
1. 函数级：检查函数内部的去重逻辑
2. 系统级：检查多个系统是否对同一批数据执行相同处理
3. 架构级：绘制事件流路径图，检查是否有环路或重复路径

#### D44：测试设计反模式检测（强制）

**触发条件**：编写新测试、修复测试失败、架构重构导致测试破坏

**反模式清单**：
1. **直接调用内部函数**：测试直接调用 `processDestroyTriggers`/`processAffectTriggers` 等内部实现，而不是通过 `runCommand`
2. **绕过 Pipeline**：测试不经过完整的命令执行流程，导致系统钩子（`afterEvents`/`postProcessSystemEvents`）未被调用
3. **验证中间状态**：测试断言内部函数的返回值，而不是最终的游戏状态
4. **假设实现细节**：测试依赖"交互在哪个阶段创建"等实现细节，而不是"交互最终是否存在"

**正确做法**：
1. **使用公开 API**：通过 `runCommand` 执行命令，让 Pipeline 自动调用所有系统
2. **验证最终状态**：断言 `finalState.sys.interaction.current` 等最终状态，而不是中间步骤
3. **黑盒测试**：测试"输入→输出"，不关心内部如何实现
4. **架构无关**：架构重构不应破坏测试（除非行为真的变了）

**判定标准**：
- 测试调用 `runCommand` → ✅ 正确
- 测试直接调用 `processXxx` 内部函数 → ❌ 反模式
- 测试断言 `finalState` → ✅ 正确
- 测试断言 `processXxx` 的返回值 → ❌ 反模式

**Igor 测试失败的教训**：
- 4 个测试直接调用 `processDestroyTriggers`，绕过 Pipeline
- 架构修复（移除重复后处理）导致测试破坏
- 如果测试使用 `runCommand`，架构修复不会破坏测试

## 教训总结

### 1. 审计粒度要多层次

- **函数级**：单个函数的逻辑正确性
- **模块级**：模块内部的一致性
- **系统级**：多个系统的协作正确性
- **架构级**：整体架构的合理性

### 2. 重构要彻底

- 引入新系统时，必须明确新旧系统的职责边界
- 如果新系统替代旧系统，必须完全移除旧系统
- 如果新旧系统并存，必须文档化职责划分

### 3. 历史债务要主动识别

- 定期审查"是否有多个系统做同样的事情"
- 定期审查"是否有遗留代码未清理"
- 定期审查"架构演进是否一致"

### 4. 测试要覆盖架构层

- 不仅要测试"功能是否正确"
- 还要测试"是否有重复处理"
- 还要测试"系统协作是否正确"

## 测试失败分析

### 失败原因

修复后，4 个测试失败：
1. `igor-two-igors-one-destroyed.test.ts` - 期望 Big Gulp 创建交互，但没有
2. `igor-ondestroy-idempotency.test.ts` (2个) - 直接调用 `processDestroyTriggers` 期望创建交互
3. `igor-double-trigger-bug.test.ts` - 直接调用 `processDestroyTriggers` 期望创建交互

**根本原因**：这些测试直接调用底层函数（`processDestroyTriggers`），绕过了 Pipeline，因此 `postProcessSystemEvents` 没有被调用，交互没有被创建。

### 测试设计问题

**反模式**：测试直接调用内部实现函数（`processDestroyTriggers`），而不是通过公开 API（`runCommand`）。

**后果**：
- 测试与实现耦合过紧
- 架构重构会破坏测试
- 测试无法验证完整的事件流

**正确做法**：
- 使用 `runCommand` 执行完整的命令流程
- 让 Pipeline 自动调用所有系统（包括 `postProcessSystemEvents`）
- 测试验证最终状态，而不是中间步骤

### 修复方案

**方案 A：修改测试使用 `runCommand`（推荐）**
- 优点：测试验证完整流程，不依赖内部实现
- 缺点：需要重写测试

**方案 B：在测试中手动调用 `postProcessSystemEvents`**
- 优点：最小改动
- 缺点：测试仍然依赖内部实现，未来可能再次破坏

**方案 C：恢复 `SmashUpEventSystem` 的后处理（不推荐）**
- 优点：测试通过
- 缺点：重新引入 bug，架构问题未解决

## 后续行动

### 立即行动

1. ✅ 修复 Igor 双重触发 bug（移除 `SmashUpEventSystem` 的后处理）
2. ⬜ 修复破坏的测试（使用方案 A：重写为 `runCommand` 测试）
3. ⬜ 添加架构测试（检测重复调用）

### 短期行动

1. ⬜ 更新审计文档（添加 D41/D42/D43 维度）
2. ⬜ 文档化 Pipeline 事件流路径
3. ⬜ 审查其他游戏是否有类似问题
4. ⬜ 建立测试最佳实践文档（禁止直接调用内部函数）

### 长期行动

1. ⬜ 文档化 Pipeline 事件处理规范（明确哪些系统在哪个阶段处理事件）
2. ⬜ 建立架构演进规范（新增系统时的检查清单）
3. ⬜ 审查所有测试，识别直接调用内部函数的反模式
4. ⬜ 定期审查"是否有多个系统做同样的事情"（每季度一次架构审计）
